{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop we will be using NLTK library to walk you through some basic steps of a text analysis project. NLTK is one of the most popular tools to process human language data. \n",
    "\n",
    "Some basic steps of text analysis we are going to demonstrate include:\n",
    "\n",
    "       -tokenize text\n",
    "       -clean punctuations \n",
    "       -remove stop words \n",
    "       -stemm words \n",
    "       -tag words \n",
    "       -vocabulary diversity\n",
    "       -word frequency distribution\n",
    "       -sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import files\n",
    "\n",
    "In order to import a folder of files, we use the os.chdir function to first navigate to the right directory.\n",
    "\n",
    "Then we use glob.glob function to iterate through all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (os.getcwd()) #getcwd helps check if we are at the right directory\n",
    "\n",
    "my_dir =\"Sample_data\"\n",
    "os.chdir(my_dir)   #change the current working directory to specified path. \n",
    "\n",
    "reviewList=[]\n",
    "for files in glob.glob(\"*.txt\"):   #glob.glob returns a list of pathnames. It helps us loop through all files\n",
    "    df = pd.read_csv(files)\n",
    "    for content in df:\n",
    "#         print (content) #read reviews as strings\n",
    "        reviewList.append(content)\n",
    "print (reviewList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the review list into a huge string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "str1= \" \"\n",
    "data = str1.join(reviewList)\n",
    "data = data.replace(\"<br />\",\"\")\n",
    "# print (data)\n",
    "\n",
    "# tokens = nltk.word_tokenize(str(words)) #convert words from list to string and tokenize them\n",
    "# print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation and stop words\n",
    "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
    "\n",
    "http://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re, pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"$?:!.,;/\\\\&*)--(...-``''\"\n",
    "\n",
    "# tokens = word_tokenize(data)\n",
    "tokens = word_tokenize(data.replace(\".\",\" \"))\n",
    "words =[word.lower() for word in tokens if not word in punctuations]\n",
    "print (sorted(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "## Add extra stop words after viewing the results\n",
    "stop_words.add(\"m\")\n",
    "stop_words.add(\"'s\")\n",
    "# stop_words.remove(\"yourself\")\n",
    "# print (sorted(stop_words))\n",
    "\n",
    "filtered_words = [word for word in words if not word in stop_words]\n",
    "print (sorted(filtered_words)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization\n",
    "\n",
    "To reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
    "\n",
    "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
    "\n",
    "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "stemmer2 =SnowballStemmer(\"english\")\n",
    "\n",
    "stem_words = [stemmer2.stem(words) for words in filtered_words]\n",
    "print (stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "print (wnl.lemmatize(\"cats\"))\n",
    "print (wnl.lemmatize(\"giggling\"))\n",
    "print (wnl.lemmatize(\"giggling\", \"v\"))\n",
    "\n",
    "# lemm_words = [wnl.lemmatize(word) for word in filtered_words]\n",
    "# print (sorted(lemm_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CC   coordinating conjunction\n",
    "    CD   cardinal digit\n",
    "    DT   determiner\n",
    "    EX   existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "    FW   foreign word\n",
    "    IN   preposition/subordinating conjunction\n",
    "    JJ   adjective 'big'\n",
    "    JJR   adjective, comparative 'bigger'\n",
    "    JJS   adjective, superlative 'biggest'\n",
    "    LS   list marker 1)\n",
    "    MD   modal could, will\n",
    "    NN   noun, singular 'desk'\n",
    "    NNS   noun plural 'desks'\n",
    "    NNP   proper noun, singular 'Harrison'\n",
    "    NNPS   proper noun, plural 'Americans'\n",
    "    PDT   predeterminer 'all the kids'\n",
    "    POS   possessive ending parent's\n",
    "    PRP   personal pronoun I, he, she\n",
    "    PRP$   possessive pronoun my, his, hers\n",
    "    \n",
    "    RB   adverb very, silently\n",
    "    RBR   adverb, comparative better\n",
    "    UH   interjection errrrrrrrm\n",
    "    VB   verb, base form take\n",
    "    VBD   verb, past tense took\n",
    "    VBG   verb, gerund/present participle taking\n",
    "    VBN   verb, past participle taken\n",
    "    VBP   verb, sing. present, non-3d take\n",
    "    VBZ   verb, 3rd person sing. present takes\n",
    "    WDT   wh-determiner which\n",
    "    WP   wh-pronoun who, what\n",
    "    WP$   possessive wh-pronoun whose\n",
    "    WRB   wh-abverb where, when\n",
    "    RBS   adverb, superlative best\n",
    "    RP   particle give up\n",
    "    TO   to go 'to' the store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use tags for lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pos_tag gets the tag for the word, it comes in form of a list of tuples[(word1, tag1)(word2, tag2)(word3, tag3)].\n",
    "\n",
    "Use indexing to drill down: the first[0] gets to the individual tuples, the [1] gets to the tags, and the [0] grabs the first letter of a tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet  \n",
    "\n",
    "#WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs \n",
    "#are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. \n",
    "\n",
    "print (nltk.pos_tag([\"meaningful\"])[0][1][0])\n",
    "       \n",
    "       \n",
    "def get_wordnet_pos(word):\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# print (get_wordnet_pos(\"facing\"))\n",
    "# print (get_wordnet_pos(\"kindly\"))\n",
    "\n",
    "\n",
    "lemm_words = [wnl.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words]\n",
    "print (sorted(lemm_words)[220:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tagging our data\n",
    "\n",
    "nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTKâ€™s POS tags to the format wordnet lemmatizer would accept. The get_wordnet_pos() function defined below does this mapping job.\n",
    "\n",
    "Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize(\"faking a review for tagging purpose\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "tags = nltk.pos_tag(lemm_words)\n",
    "# print (tags[:5])\n",
    "\n",
    "tag_counts = Counter(tag for word,tag in tags)\n",
    "print (tag_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check Vocabulary Diversity\n",
    "set() creates a distinct collection of the iterable elements (all words here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(set(lemm_words)))  # number of distinct words\n",
    "print (len(lemm_words))    # number of total words\n",
    "print (\"The vocabulary diversity of the reviews is: \"+ str(len(set(lemm_words))/len(lemm_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Count total words and unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store uniques words and their counts.\n",
    "count = {}\n",
    "for w in lemm_words:\n",
    "    if w in count:\n",
    "        count[w] += 1\n",
    "    else:\n",
    "        count[w] = 1\n",
    "    \n",
    "# count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)}\n",
    "print (count)\n",
    "\n",
    "# \"key=lambda\" allows us to sort our dictionary by value. \n",
    "# This is an example of a Lambda function, which is a function without a name.\n",
    "# default it's ascending sort, \"reverse=True\" flips the order to descending.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. NLTK's Frequency Distributions Functions\n",
    "\n",
    "    fdist = FreqDist(samples)\tcreate a frequency distribution containing the given samples\n",
    "    fdist[sample] += 1\tincrement the count for this sample\n",
    "    fdist['monstrous']\tcount of the number of times a given sample occurred\n",
    "    fdist.freq('monstrous')\tfrequency of a given sample\n",
    "    fdist.N()\ttotal number of samples\n",
    "    fdist.most_common(n)\tthe n most common samples and their frequencies\n",
    "    for sample in fdist:\titerate over the samples\n",
    "    fdist.max()\tsample with the greatest count\n",
    "    fdist.tabulate()\ttabulate the frequency distribution\n",
    "    fdist.plot()\tgraphical plot of the frequency distribution\n",
    "    fdist.plot(cumulative=True)\tcumulative plot of the frequency distribution\n",
    "    fdist1 |= fdist2\tupdate fdist1 with counts from fdist2\n",
    "    fdist1 < fdist2\ttest if samples in fdist1 occur less frequently than in fdist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_words=FreqDist(lemm_words)\n",
    "freq_words.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 5))  \n",
    "plt.title(\"Cummulative Frequency Distribution\")\n",
    "\n",
    "plot1 = FreqDist(lemm_words).plot(30, cumulative=True, color=\"black\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure(figsize=(10, 5))  \n",
    "plt.title(\"Non-cummulative Frequency Distribution\")\n",
    "plot2 = FreqDist(lemm_words).plot(30, cumulative=False, color=\"purple\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dispersion Plot\n",
    "\n",
    "Show the location of words in the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.draw.dispersion import dispersion_plot\n",
    "dispersion_plot(lemm_words, ['movie', 'scene','character'])\n"
   ]
  },
  
  
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: \n",
    "\n",
    "        https://www.nltk.org/book/ch01.html\n",
    "        https://www.nltk.org/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
