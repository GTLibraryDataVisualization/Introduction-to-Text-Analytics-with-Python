{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GTLibraryDataVisualization/Introduction-to-Text-Analytics-with-Python/blob/master/HandsOn_Text_Analytics_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fVv22izm1Ie"
      },
      "source": [
        "This workshop we will be using the NLTK library to walk you through some basic steps of a text analytics project. NLTK is a library used to work with human language data.\n",
        "\n",
        "\"Text analytics is the automated process of translating large volumes of unstructured text into quantitative data to uncover insights, trends, and patterns. Combined with data visualization tools, this technique enables companies to understand the story behind the numbers and make better decisions.\" -monkeylearn.com\n",
        "\n",
        "Some basic steps of text analysis we are going to demonstrate include:\n",
        "\n",
        "       -tokenize text\n",
        "       -clean punctuations \n",
        "       -remove stop words \n",
        "       -stemm words \n",
        "       -tag words \n",
        "       -vocabulary diversity\n",
        "       -word frequency distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMo9jMjzm1Ih"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import glob \n",
        "# import sys "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSxMor5vm1Ii"
      },
      "source": [
        "### Import files\n",
        "\n",
        "Make sure to import sample_data into Google Colab in files.\n",
        "\n",
        "In order to import a folder of files, we use the os.chdir function to first navigate to the right directory.\n",
        "\n",
        "Then we use glob.glob function to iterate through all files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ngBGcyMm1Ii"
      },
      "outputs": [],
      "source": [
        "my_dir = \"sample_data\"\n",
        "os.chdir(my_dir)   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450u00VYaz-C"
      },
      "outputs": [],
      "source": [
        "reviewList=[]\n",
        "#Your Code Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsaj5MSKm1Ij"
      },
      "source": [
        "We want a bag of words, so we convert the review list into a huge string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJL8CQZ1m1Ij",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "str1 = \" \"\n",
        "#Your Code Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V57_YF5m1Ik"
      },
      "source": [
        "### Remove punctuation and stop words\n",
        "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
        "\n",
        "Stop words are words that are so commonly used that they carry very little useful information.\n",
        "\n",
        "We will be using the NLTK to help us.\n",
        "http://www.nltk.org/nltk_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2tvpESTm1Ik"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download_shell() for mac users\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from string import punctuation\n",
        "from nltk.tokenize import word_tokenize "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgQpOXZxm1Il"
      },
      "outputs": [],
      "source": [
        "#Your Code Here\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "6cx7Yk22nbiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqDfBOaum1Im"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words\n",
        "#Your Code Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mbhM-Hzm1In"
      },
      "source": [
        "### Stemming and Lemmatization\n",
        "\n",
        "To reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5CfmINPm1In"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet') #lexical database for English (nouns, adjectives, adverbs, verbs)"
      ],
      "metadata": {
        "id": "o6lcKIRcnmI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMSUP7RDm1Io"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfYcsQuVm1Io"
      },
      "source": [
        "### Speech Tagging\n",
        "Words in a sentence can be categorized by their syntatic function, known as the part of speech (POS). Take a look at the table below to see some examples of POS tags. We can use the tag to help our lemmatizer to return a word to its original form.\n",
        "\n",
        "Tag| Definition | Example\n",
        "--- | --- | ---\n",
        "CC | coordinating conjunction | and, but, for, etc\n",
        "CD | cardinal digit | 0, 10, 523\n",
        "DT | determiner | that, which\n",
        "EX | existential there | \"there is\" ... think of it like \"there exists\"\n",
        "FW | foreign word | \n",
        "IN | preposition/subordinating conjunction | above, toward, on, etc\n",
        "JJ | adjective | big \n",
        "JJR | adjective, comparative |bigger\n",
        "JJS | adjective, superlative | biggest\n",
        "LS | list marker | 1)\n",
        "MD | modal | could, will\n",
        "NN | noun, singular | desk\n",
        "NNS | noun plural | desks\n",
        "NNP | proper noun, singular | Harrison\n",
        "NNPS | proper noun, plural | Americans\n",
        "PDT | predeterminer | all the kids\n",
        "POS | possessive ending | parent's\n",
        "PRP | personal pronoun | I, he, she\n",
        "PRP$ |  possessive pronoun | my, his, hers\n",
        "RB | adverb | very, silently\n",
        "RBR |  adverb, comparative | better\n",
        "UH | interjection | errrrrrrrm\n",
        "VB | verb, base form | take\n",
        "VBD | verb, past tense | took\n",
        "VBG | verb, gerund/present participle | taking\n",
        "VBN | verb, past participle | taken\n",
        "VBP | verb, present tense non-3rd person singular | take\n",
        "VBZ | verb, present tense 3rd person singular | takes\n",
        "WDT | wh-determiner | which\n",
        "WP | wh-pronoun | who, what\n",
        "WP\\$ | possessive wh-pronoun | whose\n",
        "WRB | wh-abverb | where, when\n",
        "RBS  | adverb, superlative | best\n",
        "RP  | particle | give up\n",
        "TO  | infinitive marker| go 'to' the store\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbu-C_Qm1Iq"
      },
      "source": [
        "#### 1. Tagging our data\n",
        "\n",
        "nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTKâ€™s POS tags to the format wordnet lemmatizer would accept. The get_wordnet_pos() function defined below does this mapping job.\n",
        "\n",
        "Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbdJvCs9m1Iq"
      },
      "outputs": [],
      "source": [
        "text = word_tokenize(\"faking a review for tagging purpose\")\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A98p2PVCm1Iq"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "#Your Code Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3UAyId0m1Ip"
      },
      "source": [
        "#### 2. Use tags for lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIPHNY7Lm1Ip"
      },
      "source": [
        "pos_tag gets the tag for the word, it comes in form of a list of tuples[(word1, tag1)(word2, tag2)(word3, tag3)]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsCQaU0vm1Ip",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet  \n",
        "       \n",
        "def get_wordnet_pos(tokens: list) -> list:\n",
        "    tag_dict = {\n",
        "                \"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV\n",
        "                }\n",
        "    tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    return [(word, tag_dict.get(tag[0], wordnet.NOUN)) for word, tag in tags]\n",
        "\n",
        "\n",
        "#Your Code Here\n",
        "pos_str = \"The quick brown fox jumped over the lazy dog\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfX9Mqqem1Ir"
      },
      "source": [
        "### Counting Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arzyob5Cm1Ir"
      },
      "source": [
        "#### 1. Check Vocabulary Diversity\n",
        "set() creates a distinct collection of the iterable elements (all words here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIXlmiHim1Ir"
      },
      "outputs": [],
      "source": [
        "distinct_words = len(set(lemm_words))\n",
        "total_words = len(lemm_words)\n",
        "\n",
        "print (distinct_words)  # number of distinct words\n",
        "print (total_words)    # number of total words\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCWRRMZm1Ir"
      },
      "source": [
        "#### 2. Count total words and unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdf31OEzm1Is"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to store uniques words and their counts.\n",
        "count = {}\n",
        "#Your Code Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5OyoKi4m1Is"
      },
      "source": [
        "#### 3. NLTK's Frequency Distributions Functions\n",
        "\n",
        "We can initiate a frequency distribution by inputing our samples (a list of words) into `FreqDist` \n",
        "\n",
        "`fdist = FreqDist(samples)   #create a frequency distribution containing the given samples`\n",
        "\n",
        "Function | Description\n",
        "--- | ---\n",
        "`fdist[sample] += 1`\t| increment the count for this sample\n",
        "`fdist['monstrous']`\t| count of the number of times a given sample occurred\n",
        "`fdist.freq('monstrous')`\t| frequency of a given sample\n",
        "`fdist.N()`\t| total number of samples\n",
        "`fdist.most_common(n)`\t| the n most common samples and their frequencies\n",
        "`for sample in fdist:`\t| iterate over the samples\n",
        "`fdist.max()`\t| sample with the greatest count\n",
        "`fdist.tabulate()`\t| tabulate the frequency distribution\n",
        "`fdist.plot()`\t| graphical plot of the frequency distribution\n",
        "`fdist.plot(cumulative=True)`\t| cumulative plot of the frequency distribution\n",
        "`fdist1 \\|= fdist2`\t| update fdist1 with counts from fdist2\n",
        "`fdist1 < fdist2`   | test if samples in fdist1 occur less frequently than in fdist2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIMrS_BKm1Is"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist #frequency distribution\n",
        "\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoG0TKxCm1Is"
      },
      "outputs": [],
      "source": [
        "! pip install matplotlib #creating static, animated, and interactive visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7CsFXMNm1Is"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt #basically matlab\n",
        "\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HMkHVEOm1It"
      },
      "source": [
        "### Dispersion Plot\n",
        "\n",
        "Show the location of words in the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP8yETx8m1It"
      },
      "outputs": [],
      "source": [
        "from nltk.draw.dispersion import dispersion_plot \n",
        "\n",
        "#Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuw0eaBHm1It"
      },
      "source": [
        "Reference: \n",
        "\n",
        "        https://www.nltk.org/book/ch01.html\n",
        "        https://www.nltk.org/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text Analytics with Python Hands On.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf3568ae63ea849c56f739bf537ac65f18326a99a87e904acc30f8f69eaac0d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}