{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GTLibraryDataVisualization/Introduction-to-Text-Analytics-with-Python/blob/master/Final_Text_Analytics_with_Python_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fVv22izm1Ie"
      },
      "source": [
        "This workshop we will be using the NLTK library to walk you through some basic steps of a text analytics project. NLTK is a library used to work with human language data.\n",
        "\n",
        "\"Text analytics is the automated process of translating large volumes of unstructured text into quantitative data to uncover insights, trends, and patterns. Combined with data visualization tools, this technique enables companies to understand the story behind the numbers and make better decisions.\" -monkeylearn.com\n",
        "\n",
        "NLTK is one of the most popular tools to process human language data. \n",
        "\n",
        "Some basic steps of text analysis we are going to demonstrate include:\n",
        "\n",
        "       -tokenize text\n",
        "       -clean punctuations \n",
        "       -remove stop words \n",
        "       -stem and lemmatize words \n",
        "       -tag words \n",
        "       -vocabulary diversity\n",
        "       -word frequency distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMo9jMjzm1Ih"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "from pathlib import Path #provides an object api for working with files and directories\n",
        "import os #provides functions for interacting with operating systems\n",
        "import glob #used to return all file paths that match a specific pattern\n",
        "import sys #provides functions and variables used to manipulate different part of the Python runtime environment"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MSxMor5vm1Ii"
      },
      "source": [
        "### Import files\n",
        "\n",
        "Make sure to import sample_data into Google Colab in files.\n",
        "\n",
        "In order to import a folder of files, we use the os.chdir function to first navigate to the right directory.\n",
        "\n",
        "Then we use glob.glob function to iterate through all files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "3ngBGcyMm1Ii",
        "outputId": "0cbef5eb-8dd9-47b3-9937-7de450021666"
      },
      "outputs": [],
      "source": [
        "my_dir = \"sample_data\"\n",
        "os.chdir(my_dir)   # change the current working directory to specified path. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.getcwd() # verify that we are in the right directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450u00VYaz-C",
        "outputId": "8258ad24-b772-471f-de19-8f6344e1e99f"
      },
      "outputs": [],
      "source": [
        "reviewList=[]\n",
        "# code through here\n",
        "for file in glob.glob(\"*.txt\"):   # glob.glob returns a list of pathnames. It helps us loop through all files with a .txt extension \n",
        "    with open(file, \"r\") as f:\n",
        "        content = f.readlines()\n",
        "        for line in content:\n",
        "            reviewList.append(line) # add all the data (or in this case the strings in the files in sample data) to this list\n",
        "print (reviewList) # see the list of Strings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsaj5MSKm1Ij"
      },
      "source": [
        "We want a bag of words, so we convert the review list into a huge string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EJL8CQZ1m1Ij",
        "outputId": "51e5a5d0-9564-4134-9984-a9c69de783a8",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "str1 = \" \" # String that will combine all the strings in the reviewList into one huge string\n",
        "data = str1.join(reviewList)\n",
        "\n",
        "data = data.replace(\"<br />\",\"\") # deletes any breaks...\n",
        "data = data.replace(\"\\n\", \"\") # and \\n (newline characters)\n",
        "data = data.replace(\".\",\" \") # Remove sentence structure by removing periods\n",
        "\n",
        "data = data.lower() # Normalize any capitalization\n",
        "\n",
        "print (data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V57_YF5m1Ik"
      },
      "source": [
        "### Remove punctuation and stop words\n",
        "Tokenization is the process by which big quantity of text is divided into smaller parts called tokens.\n",
        "\n",
        "Stop words are words that are so commonly used that they carry very little useful information.\n",
        "\n",
        "We will be using the NLTK to help us.\n",
        "http://www.nltk.org/nltk_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2tvpESTm1Ik"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "# nltk.download_shell() for mac users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMtYkTZm1Il"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords # stopwords are words that can be safely ignored, they don't add much meaning to a sentence\n",
        "from string import punctuation # contains all punctuation characters\n",
        "from nltk.tokenize import word_tokenize # splits a string into individual words called tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgQpOXZxm1Il",
        "outputId": "22f0bb07-9e1a-4468-966a-b36b07cd328b"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize(data) # converts string into a list of tokens (words)\n",
        "words = [word for word in tokens if not word in punctuation]\n",
        "print(sorted(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqDfBOaum1Im",
        "outputId": "6b5a9cc0-34fd-4a0a-9f5d-f3af81ff972d"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "## Add extra stop words after viewing the results\n",
        "# stop_words.add(\"'m\")\n",
        "# stop_words.add(\"'s\")\n",
        "# stop_words.add(\"''\")\n",
        "# stop_words.add(\"'ll\")\n",
        "# stop_words.add(\"``\")\n",
        "# stop_words.remove(\"yourself\")\n",
        "# print (sorted(stop_words))\n",
        "\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(sorted(filtered_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mbhM-Hzm1In"
      },
      "source": [
        "### Stemming and Lemmatization\n",
        "\n",
        "To reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n",
        "\n",
        "Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes. \n",
        "\n",
        "Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5CfmINPm1In",
        "outputId": "acdd8be6-a655-4b84-83d5-1fabd9b1965b"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer \n",
        "#process for removing the commoner morphological and inflexional endings from words in English, liked,likes->like\n",
        "from nltk.stem import SnowballStemmer\n",
        "#improved PorterStemmmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmer2 = SnowballStemmer(\"english\") #tell SnowballStemmer the language is English\n",
        "\n",
        "stem_words = [stemmer2.stem(words) for words in filtered_words]\n",
        "print (sorted(stem_words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMSUP7RDm1Io",
        "outputId": "1e4cee01-1c06-4239-a45c-ddfff0954de4"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet') #lexical database for English (nouns, adjectives, adverbs, verbs)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "#the process of grouping together the different inflected forms of a word so they can be analyzed as a single item\n",
        "wnl = WordNetLemmatizer()\n",
        "print(wnl.lemmatize(\"cats\"))\n",
        "print(wnl.lemmatize(\"giggling\"))\n",
        "print(wnl.lemmatize(\"giggling\", \"v\"))\n",
        "\n",
        "lemm_words = [wnl.lemmatize(word) for word in filtered_words]\n",
        "print(sorted(lemm_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq22eaVNm1Io"
      },
      "source": [
        "### Speech Tagging\n",
        "Words in a sentence can be categorized by their syntatic function, known as the part of speech (POS). Take a look at the table below to see some examples of POS tags. We can use the tag to help our lemmatizer to return a word to its original form.\n",
        "\n",
        "Tag| Definition | Example\n",
        "--- | --- | ---\n",
        "CC | coordinating conjunction | and, but, for, etc\n",
        "CD | cardinal digit | 0, 10, 523\n",
        "DT | determiner | that, which\n",
        "EX | existential there | \"there is\" ... think of it like \"there exists\"\n",
        "FW | foreign word | \n",
        "IN | preposition/subordinating conjunction | above, toward, on, etc\n",
        "JJ | adjective | big \n",
        "JJR | adjective, comparative |bigger\n",
        "JJS | adjective, superlative | biggest\n",
        "LS | list marker | 1)\n",
        "MD | modal | could, will\n",
        "NN | noun, singular | desk\n",
        "NNS | noun plural | desks\n",
        "NNP | proper noun, singular | Harrison\n",
        "NNPS | proper noun, plural | Americans\n",
        "PDT | predeterminer | all the kids\n",
        "POS | possessive ending | parent's\n",
        "PRP | personal pronoun | I, he, she\n",
        "PRP$ |  possessive pronoun | my, his, hers\n",
        "RB | adverb | very, silently\n",
        "RBR |  adverb, comparative | better\n",
        "UH | interjection | errrrrrrrm\n",
        "VB | verb, base form | take\n",
        "VBD | verb, past tense | took\n",
        "VBG | verb, gerund/present participle | taking\n",
        "VBN | verb, past participle | taken\n",
        "VBP | verb, present tense non-3rd person singular | take\n",
        "VBZ | verb, present tense 3rd person singular | takes\n",
        "WDT | wh-determiner | which\n",
        "WP | wh-pronoun | who, what\n",
        "WP$ | possessive wh-pronoun | whose\n",
        "WRB | wh-abverb | where, when\n",
        "RBS  | adverb, superlative | best\n",
        "RP  | particle | give up\n",
        "TO  | infinitive marker| go 'to' the store\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbu-C_Qm1Iq"
      },
      "source": [
        "#### 1. Tagging our data\n",
        "\n",
        "nltk.pos_tag() returns a tuple with the POS tag. The key here is to map NLTKâ€™s POS tags to the format wordnet lemmatizer would accept. The get_wordnet_pos() function defined below does this mapping job.\n",
        "\n",
        "Reference: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbdJvCs9m1Iq",
        "outputId": "371c20d7-4c6b-4c35-b79c-74da0c93bce7"
      },
      "outputs": [],
      "source": [
        "text = word_tokenize(\"faking a review for tagging purpose\")\n",
        "nltk.pos_tag(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A98p2PVCm1Iq",
        "outputId": "47fbf875-7e02-4e21-b4d6-a686e4e6d766"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "#count hashable objects\n",
        "\n",
        "tags = nltk.pos_tag(filtered_words)\n",
        "# print (tags[:5])\n",
        "\n",
        "tag_counts = Counter(tag for word,tag in tags)\n",
        "print (tag_counts)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "y3UAyId0m1Ip"
      },
      "source": [
        "#### 2. Use tags for lemmatization\n",
        "\n",
        "pos_tag gets the tag for the word, it comes in form of a list of tuples[(word1, tag1)(word2, tag2)(word3, tag3)].\n",
        "\n",
        "Use indexing to drill down: the first[0] gets to the individual tuples, the [1] gets to the tags, and the [0] grabs the first letter of a tag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsCQaU0vm1Ip",
        "outputId": "47f8c2fc-2e42-4985-8799-c0e91602da5f",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "#tuples are collections of objects in order\n",
        "from nltk.corpus import wordnet  \n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "#WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs \n",
        "#are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. \n",
        "\n",
        "print(nltk.pos_tag([\"meaningful\"])[0][1][0])\n",
        "#is wrong, meaning automatically imported pretrained model is not good\n",
        "\n",
        "#We are tagging each word so we can lemmatize it to common base form\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "print (get_wordnet_pos(\"facing\"))\n",
        "print (get_wordnet_pos(\"kindly\"))\n",
        "\n",
        "lemm_words = [wnl.lemmatize(w, get_wordnet_pos(w)) for w in filtered_words]\n",
        "print (sorted(lemm_words)[:400])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IfX9Mqqem1Ir"
      },
      "source": [
        "### Counting Words\n",
        "#### 1. Check Vocabulary Diversity\n",
        "set() creates a distinct collection of the iterable elements (all words here)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIXlmiHim1Ir",
        "outputId": "59f33556-f630-451f-8398-fc1cca73dc1d"
      },
      "outputs": [],
      "source": [
        "distinct_words = len(set(lemm_words))\n",
        "total_words = len(lemm_words)\n",
        "vocab_diversity = distinct_words / total_words\n",
        "print (distinct_words)  # number of distinct words\n",
        "print (total_words)    # number of total words\n",
        "print (f\"The vocabulary diversity of the reviews is: {vocab_diversity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UCWRRMZm1Ir"
      },
      "source": [
        "#### 2. Count total words and unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdf31OEzm1Is",
        "outputId": "33c994ac-cf53-45eb-f04e-5305538f8e6e"
      },
      "outputs": [],
      "source": [
        "# create a dictionary to store uniques words and their counts.\n",
        "count = {}\n",
        "for w in lemm_words:\n",
        "    #count[w] = count.get(w, 1)\n",
        "    if w in count:\n",
        "        count[w] += 1\n",
        "    else:\n",
        "        count[w] = 1\n",
        "    \n",
        "count = {k: v for k, v in sorted(count.items(), key=lambda item: item[1], reverse=True)} \n",
        "#count has two items: key and value\n",
        "#word is the key and count is the value\n",
        "#we are sorting from greatest to smallest\n",
        "\n",
        "print (count)\n",
        "\n",
        "# \"key=lambda\" allows us to sort our dictionary by value. \n",
        "# This is an example of a Lambda function, which is a function without a name.\n",
        "# default it's ascending sort, \"reverse=True\" flips the order to descending."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5OyoKi4m1Is"
      },
      "source": [
        "#### 3. NLTK's Frequency Distributions Functions\n",
        "\n",
        "We can initiate a frequency distribution by inputing our samples (a list of words) into `FreqDist` \n",
        "\n",
        "`fdist = FreqDist(samples)   #create a frequency distribution containing the given samples`\n",
        "\n",
        "Function | Description\n",
        "--- | ---\n",
        "`fdist[sample] += 1`\t| increment the count for this sample\n",
        "`fdist['monstrous']`\t| count of the number of times a given sample occurred\n",
        "`fdist.freq('monstrous')`\t| frequency of a given sample\n",
        "`fdist.N()`\t| total number of samples\n",
        "`fdist.most_common(n)`\t| the n most common samples and their frequencies\n",
        "`for sample in fdist:`\t| iterate over the samples\n",
        "`fdist.max()`\t| sample with the greatest count\n",
        "`fdist.tabulate()`\t| tabulate the frequency distribution\n",
        "`fdist.plot()`\t| graphical plot of the frequency distribution\n",
        "`fdist.plot(cumulative=True)`\t| cumulative plot of the frequency distribution\n",
        "`fdist1 \\|= fdist2`\t| update fdist1 with counts from fdist2\n",
        "`fdist1 < fdist2`   | test if samples in fdist1 occur less frequently than in fdist2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIMrS_BKm1Is",
        "outputId": "26508a17-963d-4dc1-c094-e7be524e0779"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist #frequency distribution\n",
        "\n",
        "freq_words=FreqDist(lemm_words) # intialize the freq distribution on our sample of lemmatized words\n",
        "freq_words.most_common(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BoG0TKxCm1Is",
        "outputId": "93ecf4c9-2e96-44a5-fb05-134744a92bdf"
      },
      "outputs": [],
      "source": [
        "! pip install matplotlib #creating static, animated, and interactive visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "y7CsFXMNm1Is",
        "outputId": "8547e808-d618-4fe8-b66c-968906129aec"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt #basically matlab\n",
        "#plt.figure(figsize=(12, 5))  #(x,y)\n",
        "#plt.title(\"Cummulative Frequency Distribution\")\n",
        "plot1 = FreqDist(lemm_words).plot(30, cumulative=True, color=\"black\") #frequency distribution\n",
        "\n",
        "plt.figure(figsize=(12, 5))  \n",
        "plt.title(\"Non-cummulative Frequency Distribution\")\n",
        "plot2 = FreqDist(lemm_words).plot(30, cumulative=False, color=\"purple\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HMkHVEOm1It"
      },
      "source": [
        "### Dispersion Plot\n",
        "\n",
        "Show the location of words in the collection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "AP8yETx8m1It",
        "outputId": "71ac412b-bbea-4f79-c28c-7b020135e845"
      },
      "outputs": [],
      "source": [
        "from nltk.draw.dispersion import dispersion_plot \n",
        "#allows for visualization of the lexical dispersion of words in a corpus, which is a collection of texts\n",
        "dispersion_plot(lemm_words, ['movie', 'scene','character'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuw0eaBHm1It"
      },
      "source": [
        "Reference: \n",
        "\n",
        "        https://www.nltk.org/book/ch01.html\n",
        "        https://www.nltk.org/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Text Analytics with Python-Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "bf3568ae63ea849c56f739bf537ac65f18326a99a87e904acc30f8f69eaac0d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
